# H2O.ai Prague Meetup Number 4

Meetup Page: https://www.meetup.com/Prague-Artificial-Intelligence-Deep-Learning/events/264335458/

Date: Sep 19, 2019

Location: H2O.ai Prague Office

Video Link: https://www.youtube.com/watch?v=Z1IQLNLCN5c

## Agenda

### Talk 1: Customized Loss Function in Gradient Boosting Machine by Veronika Maurerova

About Veronika:

- Software Engineer at H2O.ai
- https://twitter.com/MaureVer


### Talk 2: Tableau Extensions API & Future of AI/Machine Learning by Radovan Kavický

About Radovan:

- Principal Data Scientist & President at GapData Institute
- Member of Slovak.AI, CLAIRE, European AI Alliance & Slovak Economic Association
- Data Science Instructor @ DataCamp, BaseCamp.ai, Learn2Code, Gopas, GapData
- Founder of PyData Slovakia/Bratislava (#PyDataBA), R <- Slovakia (#RSlovakia), Julia Users Group Slovakia (#JUGSlovakia) & SK/CZ Tableau User Group (#skczTUG)
- https://www.linkedin.com/in/radovankavicky
- https://github.com/radovankavicky

### Talk 3: General pipeline for Computer Vision problems by Yauhen Babakhin

In this talk, we will consider the whole process of addressing Computer Vision problems. Starting with the data preparation and validation strategy. Proceeding to the training process accompanied by some recent methods in Deep Learning. And finishing with some practical tips and tricks that could help to increase the quality of the model.

About Yauhen:

- Yauhen is a data scientist at H2O.ai. He holds a Master’s Degree in Applied Data Analysis and has over 4 years of working experience in Data Science. He worked in Banking, Gaming and eCommerce domains. He is also the first Kaggle competitions Grandmaster in Belarus having gold medals in both classic Machine Learning and Deep Learning competitions.
- https://www.linkedin.com/in/yauhenbabakhin/


### Talk 4: FastText Vector Norms And OOV Words by Vaclav Kosar

Word embeddings, trained on large unlabeled corpora are useful for many natural language processing tasks. FastText (Bojanowski et al., 2016) in contrast to Word2vec model accounts for sub-word information by also embedding sub-word n-grams. FastText word representation is the word embedding vector plus the sum of n-grams contained in it. Word2vec vector norms have been shown (Schakel & Wilson, 2015) to be correlated to word significance. This talk discusses the visualization of vector norms of FastText embeddings and evaluates the use of FastText word vector norm multiplied with the number of word n-grams for detecting non-English OOV words.

About Vaclav:
- Vaclav is a programming and ML enthusiast. He currently forges data flow software and dabbles in machine learning for Time Is Ltd. He studied electronics, physics, and mathematics.
- https://www.linkedin.com/in/vaclav-kosar-47755863/
